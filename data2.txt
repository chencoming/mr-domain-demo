The hadoopJava type runs user java program after all Upon execution it tries to construct an object that has the constructor signature of constructor String Props  and runs its run method If user wants to cancel the job it tries the user defined cancel method before doing a hard kill on that process
The hadoopJava job type talks to a secure cluster via Hadoop tokens The admin should specify obtainbinarytokentrue if the Hadoop cluster security is turned on Before executing a job Azkaban will obtain name node token and job tracker tokens for this job These tokens will be written to a token file to be picked up by user job process during its execution After the job finishes Azkaban takes care of canceling these tokens from name node and job tracker
Since Azkaban only obtains the tokens at the beginning of the job run and does not requesting new tokens or renew old tokens during the execution it is important that the job does not run longer than configured token life
If there are multiple job submissions inside the user program the user should also take care not to have a single MR step cancel the tokens upon completion thereby failing all other MR steps when they try to authenticate with Hadoop services
Vanilla Pig types dont provide all udf jars It is often up to the admin who sets up Azkaban to provide a pre-configured Pig job type with company specific udfs registered and name space imported so that the users dont need to provide all the jars and do the configurations in their specific Pig job conf files
Pig type is built on using hadoop tokens to talk to secure Hadoop clusters Therefore individual Azkaban Pig jobs are restricted to run within the tokens lifetime which is set by Hadoop admins It is also important that individual MR step inside a single Pig script doesnt cancel the tokens upon its completion Otherwise all following steps will fail on authentication with job tracker or name node
Since Azkaban only obtains the tokens at the beginning of the job run and does not request new tokens or renew old tokens during the execution it is important that the job does not run longer than configured token life It is also important that individual MR step inside a single Pig script doesnt cancel the tokens upon its completion
Since Azkaban job types are named by their directory names the admin should also make those naming public and consistent For example while there are multiple versions of Pig job types the admin can link one of them as pig for default Pig type Experimental Pig versions can be tested in parallel with a different name and can be promoted to default Pig type if it is proven stable In LinkedIn we also provide Pig job types that have a number of useful udf libraries including datafu and LinkedIn specific ones pre-registered and imported so that users in most cases will only need Pig scripts in their Azkaban job packages
Since Azkaban only obtains the tokens at the beginning of the job run and does not request new tokens or renew old tokens during the execution it is important that the job does not run longer than configured token life It is also important that individual MR step inside a single Pig script doesnt cancel the tokens upon its completion Otherwise all following steps will fail on authentication with Hadoop services
One doesnt always need to write java code to create job types for end users Often times configuration changes of existing job types would create significantly different behavior to the end users For example in LinkedIn apart from the pig types we also have pigLi types that come with all the useful library jars pre-registered and imported This way normal users only need to provide their pig scripts and the their own udf jars to Azkaban The pig job should run as if it is run on the gateway machine from pig grunt In comparison if users are required to use the basic pig job types they will need to package all the necessary jars in the Azkaban job package and do all the register and import by themselves which often poses some learning curve for new pig/Azkaban users
If one needs to create a different job type a good starting point is to see if this can be done by using an existing job type In hadoop land this most often means the hadoopJava type Essentially all hadoop jobs from the most basic mapreduce job to pig hive crunch etc are java programs that submit jobs to hadoop clusters It is usually straight forward to create a job type that takes user input and runs a hadoopJava job
For example one can take a look at the VoldemortBuildandPush job type It will take in user input such as which cluster to push to voldemort store name etc and runs hadoopJava job that does the work For end users though this is a VoldemortBuildandPush job type with which they only need to fill out the job file to push data from hadoop to voldemort stores
Azkaban will use the default notification emails set in the final job in the flow If overridden a user can change the email addresses where failure or success emails are sent The list can be delimited by commas whitespace or a semi-colon
From the Flow View panel you can right click on the graph and disable or enable jobs Disabled jobs will be skipped during execution as if their dependencies have been met Disabled jobs will appear translucent
Adding user permissions gives those users those specified permissions on the project Remove user permissions by unchecking all of the permissions
Group permissions allow everyone in a particular group the specified permissions Remove group permissions by unchecking all the group permissions
If proxy users are turned on proxy users allows the project workflows to run as those users This is useful for locking down which headless accounts jobs can proxy to They are removed by clicking on the Remove button once added
Every user is validated through the UserManager to prevent invalid users from being added Groups and Proxy users are also check to make sure they are valid and to see if the admin is allowed to add them to the project

